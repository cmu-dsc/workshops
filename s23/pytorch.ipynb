{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to the DSC Introduction to PyTorch workshop!\n",
        "\n",
        "Link to slides [here](https://docs.google.com/presentation/d/1GDByplSSWzfNlgx-BD876XsICvo9xM11CBk1TiCS9l4/edit?usp=sharing).\n",
        "\n",
        "Link to reference notebook [here](https://drive.google.com/file/d/1z9X1SKIB8ORW3Jj30yAp4SjsIBqD44b5/view?usp=sharing).\n",
        "\n"
      ],
      "metadata": {
        "id": "E9uKM7jHlou-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "VQepTIzAGXlj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E6t40qW3GMSR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchsummary import summary\n",
        "import torchvision\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "bUdRTzjMSg_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown \"1ps2tXqzOwAWlX1wRsRDfcXpr7rIitJYr&confirm=t\""
      ],
      "metadata": {
        "id": "gn_9T-V41qkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip workshop_images.zip"
      ],
      "metadata": {
        "id": "vO6U7MGd6nZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "RUuuoHv_GX9x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataloader is already implemented for you. Feel free to come back to it to edit the image_transform with data augmentations."
      ],
      "metadata": {
        "id": "Vtj4RxFeObe3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/workshop_images/data.csv\")\n",
        "df"
      ],
      "metadata": {
        "id": "X9OgaIVc6wIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test, _, _ = train_test_split(df, df[\"class_name\"], test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "bdaKB_Fo7Nxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "YlsRpF_Z7nbY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "bDTxqA2G7oWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "image_transform = T.Compose([\n",
        "    T.Resize((256,256)),\n",
        "    T.ToTensor()]\n",
        ")\n",
        "\n",
        "class TrafficSignDataset(Dataset):\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "        self.transform = image_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        path, class_name = self.df.iloc[idx][\"path_name\"], self.df.iloc[idx][\"class_name\"]\n",
        "        path = \"/content/workshop_\" + path\n",
        "        img = Image.open(path)\n",
        "        img = self.transform(img)\n",
        "        return img, class_name - 1"
      ],
      "metadata": {
        "id": "0J-eCt5i3coW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataset = TrafficSignDataset(train)\n",
        "test_dataset = TrafficSignDataset(test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "g0lr_tcf8J7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, class_name = next(iter(train_loader))\n",
        "print(img.shape)\n",
        "print(class_name)"
      ],
      "metadata": {
        "id": "4j_pfsxz8TNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Model\n",
        "\n",
        "\n",
        "In this model architecture block, we will first look at how to create a model out of linear layers.\n",
        "\n",
        "\n",
        "Refer to the PyTorch documentation often, there's no need to memorize anything!\n",
        "\n",
        "Linear layer reference. https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear\n",
        "\n",
        "Activation functions. https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
        "\n"
      ],
      "metadata": {
        "id": "PHyeYOzRGYRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearModelClass(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(LinearModelClass, self).__init__()\n",
        "\n",
        "        self.input_size = 4*256*256  # How many input pixels * channels  are in our image?\n",
        "\n",
        "        self.output_size = 4  # How many different classes are we trying to predict from?\n",
        "\n",
        "        self.hidden_size = 1024   # Choose any number you'd like for connecting the layers.\n",
        "                          # A number too small will lead to an information bottleneck.\n",
        "                          # A number too large is unnecessary and harder to train.\n",
        "                          # Suggested values: 512, 1024, 2048\n",
        "\n",
        "        self.model = torch.nn.Sequential( # This stacks multiple layers together to be called at once.\n",
        "\n",
        "            # We need this in order to read a 2D image and its color channels as a single vector\n",
        "            torch.nn.Flatten(1),\n",
        "\n",
        "            # First hidden layer\n",
        "            torch.nn.Linear(in_features=self.input_size, out_features=self.hidden_size),\n",
        "\n",
        "            # Choose an activation function (Use documenation above).\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # You may add additional hidden layers if you'd like.\n",
        "            # The in features must match the previous layer's out features,\n",
        "            #   and the out_features matches next layer's in features.\n",
        "            # Remember to add activation layers after each hidden layer.\n",
        "\n",
        "\n",
        "\n",
        "            # Final (output) layer\n",
        "            torch.nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "            # Do not add activation layers or regularization after the final layer.\n",
        "            # This will lead to information loss. PyTorch will add the softmax for us.\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.model(x)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "Q_gIV-27NVQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_01 = LinearModelClass().to(device)\n",
        "summary(model_01, (4, 256, 256))"
      ],
      "metadata": {
        "id": "NphGEBc_3b4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For the rest of this section, we recommend you skip this and come back to it after you train the LinearModelClass.**"
      ],
      "metadata": {
        "id": "kUDrflorl-AO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this model architecture block, we will improve on our old model architecture by using convolutional layers in addition to linear layers. Convolutional layers are more suited for tasks such as image classification, where locality of pixels is useful information. It also allows us to be invariant to where things appear in an image, as convolutional layers slide the same layers to every region in the image.\n",
        "\n",
        "2D Convolutional layer reference: https://pytorch.org/docs/stable/generated/torch.nn.functional.conv2d.html#torch.nn.functional.conv2d\n",
        "\n",
        "Activation functions: https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity\n",
        "\n",
        "Regularizing layers: https://pytorch.org/docs/stable/generated/torch.nn.Dropout2d.html#torch.nn.Dropout2d\n",
        "\n",
        "https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html?highlight=batchnorm#torch.nn.BatchNorm2d\n",
        "\n"
      ],
      "metadata": {
        "id": "aW6e6A91lZjr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvModelClass(torch.nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(ConvModelClass, self).__init__()\n",
        "\n",
        "        self.output_size = 4\n",
        "\n",
        "        self.scanning_layers = torch.nn.Sequential(\n",
        "\n",
        "            # This is a convolutional layer that takes in 7x7 pixels at a time and slides 4 pixels at a time\n",
        "            # The out_channels multiplies the number of nodes in the layer\n",
        "            torch.nn.Conv2d(in_channels=4, out_channels=512, kernel_size=7, stride=4),\n",
        "            # Choose an activation function.\n",
        "            torch.nn._____(),\n",
        "\n",
        "            # Add some regularization between layers as well, using either BatchNorm or Dropout\n",
        "            torch.nn._____(parameter),\n",
        "\n",
        "            # Add at least 2 more sequences of convolution layers, activations, and regularization.\n",
        "            # Channels must link with previous layer similar to how linear layers link in shape\n",
        "            # But don't confuse channels with the size of the image, as the image size is handled by PyTorch\n",
        "            torch.nn.Conv2d(512, _, _, _),\n",
        "\n",
        "\n",
        "            # Pooling Layer combines cells in some functional way. The following layer will have a smaller image size.\n",
        "            torch.nn.MaxPool2d(5),\n",
        "\n",
        "        )\n",
        "\n",
        "        # These will be linear layers leading up to your output\n",
        "        self.classification_layers = torch.nn.Sequential(\n",
        "            # Our Convolutional layers would benefit from some linear layers to process the information. Try adding some more.\n",
        "            torch.nn.Flatten(),\n",
        "\n",
        "            torch.nn.Linear(256, 1024),\n",
        "            torch.nn.ReLU(),\n",
        "\n",
        "            # Final (output) layer\n",
        "            # You might have to guess & check the input size\n",
        "            torch.nn.Linear(1024, self.output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        z = self.scanning_layers(x)\n",
        "\n",
        "        out = self.classification_layers(z)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "mbJRpvkXSZBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_02 = ConvModelClass().to(device)\n",
        "summary(model_02, (4, 256, 256))"
      ],
      "metadata": {
        "id": "H6hYdYb025pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "bqNIoYUO3Sp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first choose proper optimizers, objective functions, and learning rate schedulers.\n",
        "\n",
        "https://pytorch.org/docs/stable/nn.html#loss-functions\n",
        "\n"
      ],
      "metadata": {
        "id": "SpGArN9-5BKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you change this, make sure to rerun the cells below\n",
        "model = model_01"
      ],
      "metadata": {
        "id": "jtWLotl6YmIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size    = 32\n",
        "learning_rate = 0.01"
      ],
      "metadata": {
        "id": "AxIkgGGanpZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A certain loss function is common for multi-class classificaton. Use the documentation and the slides for reference.\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# You can experiment with this optimizer.\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "\n",
        "# You can experiment with this scheduler.\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, verbose=True)"
      ],
      "metadata": {
        "id": "dskhfZsI3dM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optional: Use Weights & Biases to track your model experiments. You will need to create an account. https://wandb.ai/"
      ],
      "metadata": {
        "id": "Z6mjXIjWS2ZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb --quiet\n",
        "import wandb"
      ],
      "metadata": {
        "id": "HW6k7pVtS1ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key=\"find this API key in your wandb settings\")"
      ],
      "metadata": {
        "id": "tlTB2iTxUT5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rerun this cell on every new experiment\n",
        "\n",
        "run = wandb.init(\n",
        "    name = \"LinearModel_01\",      # Choose a useful name describing this run\n",
        "    reinit = True,                # Creates a new run when you run this cell again\n",
        "    project = \"DSC_PT_Workshop\",  # This creates a project where you'll see multiple runs\n",
        "    config = {                    # Describe the parameters for your run. You can customize this.\n",
        "      \"batch_size\": batch_size,\n",
        "      \"hidden_size\": 256,\n",
        "      \"learning_rate\": learning_rate,\n",
        "      # Add more items if necessary.\n",
        "      }\n",
        "    )"
      ],
      "metadata": {
        "id": "apqEi0KhUiSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can define the training loop procedure.\n",
        "\n",
        "This train(...) function runs one epoch of training, which means it uses all of the data once. Let's write it now.\n",
        "\n",
        "# Essential functions for training\n",
        "\n",
        "**loss = loss_fn(outputs, labels)** calculates and returns the loss\n",
        "\n",
        "**loss.backward()** calculates the gradient\n",
        "\n",
        "**outputs = model(images)** returns prediction for the batch of images\n",
        "\n",
        "**optimizer.zero_grad()** resets the gradients to zero (should do between every gradient step)\n",
        "\n",
        "**optimizer.step()** uses the gradient to update your model's weights\n",
        "\n",
        "You can also refer to https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html."
      ],
      "metadata": {
        "id": "NpEt0Iga5L8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader, optimizer, loss_fn):\n",
        "\n",
        "    # Set the model to training mode\n",
        "    model.train()\n",
        "\n",
        "    # These are just some training metrics we'd like to measure\n",
        "    num_correct = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "\n",
        "        # Send the training data to GPU if you have it\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        labels_onehot = torch.nn.functional.one_hot(labels, num_classes=4).float()\n",
        "\n",
        "        # Zero out the gradients\n",
        "\n",
        "        # Run the input through the whole model\n",
        "\n",
        "        # Calculate the loss based on your chosen loss function\n",
        "\n",
        "        # Calculate the gradient of the loss\n",
        "\n",
        "        # Update the parameters of your model\n",
        "\n",
        "        # Count the correct predictions, and accumulate total loss to find the mean later\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "\n",
        "    # We return some metrics of the model's performance so far\n",
        "    acc = 100 * num_correct / (batch_size * len(dataloader))\n",
        "    total_loss = float(total_loss / len(dataloader))\n",
        "\n",
        "    return acc, total_loss"
      ],
      "metadata": {
        "id": "mJ50XZdRgz7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch lets you define your training loop the way you like. This means you can choose your own logging statements and can choose where to call things like the scheduler. We also like that this lets us call the wandb API in the loop, but this is optional.\n",
        "\n",
        "Choose a number of epochs. Keep in mind you can always stop the training early.\n",
        "\n",
        "This is already implemented. We are ready to train."
      ],
      "metadata": {
        "id": "7MXnqXmJWR6L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "best_acc = 0.0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    train_acc, train_loss = train(model, train_loader, optimizer, loss_fn)\n",
        "\n",
        "    print(\"\\nEpoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t Learning Rate {:.04f}\".format(\n",
        "        epoch,\n",
        "        epochs,\n",
        "        train_acc,\n",
        "        train_loss,\n",
        "        curr_lr))\n",
        "\n",
        "    # For some schedulers, different inputs or location of calling are needed.\n",
        "    scheduler.step(train_loss)\n",
        "\n",
        "    ## Uncomment this if you are using Weights & Biases\n",
        "    # wandb.log({\"train_loss\": train_loss, 'train_acc': train_acc, \"learning_rate\": curr_lr})\n",
        "\n",
        "    # Save model if val_acc is better than best recorded val_acc\n",
        "    if train_acc >= best_acc:\n",
        "        best_acc = train_acc\n",
        "        print(\"Saving model\")\n",
        "        torch.save({'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': scheduler.state_dict(),\n",
        "                    'train_acc': train_acc,\n",
        "                    'epoch': epoch}, './checkpoint.pth')\n",
        "      # wandb.save('checkpoint.pth')\n",
        "\n",
        "# run.finish()"
      ],
      "metadata": {
        "id": "HLNe3VHOYUoh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In case you run out of memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "f_TYt-Ohhxmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing"
      ],
      "metadata": {
        "id": "FJTEwFgj3S-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img = next(iter(test_loader))\n",
        "img_t = torch.FloatTensor(img[0])[0, :, :, :]\n",
        "img_o = torchvision.transforms.ToPILImage()(img_t.to('cpu'))\n",
        "img_o"
      ],
      "metadata": {
        "id": "r4EMt_eevQM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classes = ['Traffic Light', 'Speed Limit', 'Crosswalk', 'Stop Sign']\n",
        "\n",
        "img_gpu = img[0].to(device)\n",
        "out = model(img_gpu)\n",
        "prediction = torch.argmax(out, axis=1)[0]\n",
        "print(\"Prediction:\", classes[prediction], \"/ Ground Truth:\", classes[img[1][0]])"
      ],
      "metadata": {
        "id": "WV-Ke1cavVQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following is similar to the train function. We keep only the parts necessary for prediction here, and find the accuracy.\n"
      ],
      "metadata": {
        "id": "Y16wqkcn5cDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, dataloader):\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  num_correct = 0\n",
        "\n",
        "  for i, (images, labels) in enumerate(dataloader):\n",
        "\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      outputs = model(images)\n",
        "\n",
        "      num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "\n",
        "\n",
        "  acc = 100 * num_correct / (batch_size * len(dataloader))\n",
        "\n",
        "  return acc"
      ],
      "metadata": {
        "id": "mKVsuZA-3dlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = test(model, test_loader)\n",
        "print('We achieved {:.04f}% accuracy'.format(test_results))"
      ],
      "metadata": {
        "id": "bPy3ujsrBJDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlit"
      ],
      "metadata": {
        "id": "w-r0W-bfGYpR"
      }
    }
  ]
}